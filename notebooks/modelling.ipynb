{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "from pyspark.ml.regression import GeneralizedLinearRegression, RandomForestRegressor\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a spark session (which will run spark jobs)\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"MAST30034 Project 1 modelling\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .config(\"spark.driver.memory\", \"3g\")\n",
    "    .config(\"spark.executer.memory\", \"4g\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read training and test dataset\n",
    "train_sdf = spark.read.parquet(\"../data/curated/new_train\")\n",
    "test_sdf = spark.read.parquet(\"../data/curated/new_test\")\n",
    "\n",
    "# check size of training/test set\n",
    "print(\"Number of instances in traning data:\", train_sdf.count())\n",
    "print(\"Number of instances in test data:\", test_sdf.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define which attributes are numerical versus categorical\n",
    "label = \"trip_time\"\n",
    "categorical_attr = [\"day_of_week\"]\n",
    "\n",
    "# binary attribute does not require one-hot encoding\n",
    "numerical_binary_attr = [\"trip_miles\", \"Temperature (F)\", \"hour_of_day\", \"congestion_zone\", \n",
    "                         \"JFK_trip\", \"Newark_trip\", \"LaGuardia_trip\", \"Manhattan_trip\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess attributes and data in train and test sets\n",
    "def preprocess_attribute(train_sdf, test_sdf):\n",
    "\n",
    "    # apply one-hot encoding to categorical attributes\n",
    "    indexers = [StringIndexer(inputCol=c, outputCol=c+\"_index\") for c in categorical_attr]\n",
    "    encoder = OneHotEncoder(\n",
    "        inputCols=[indexer.getOutputCol() for indexer in indexers],\n",
    "        outputCols=[\n",
    "            \"{0}_encoded\".format(indexer.getOutputCol()) for indexer in indexers]\n",
    "    )\n",
    "\n",
    "    # use VectorAssembler to assemble data together as a vector\n",
    "    assembler = VectorAssembler(inputCols = encoder.getOutputCols() + numerical_binary_attr, outputCol = \"features\")\n",
    "\n",
    "    pipeline = Pipeline(stages=indexers + [encoder, assembler])\n",
    "    preprocess = pipeline.fit(train_sdf.dropna('any'))\n",
    "    model_sdf = preprocess.transform(train_sdf)\n",
    "\n",
    "    # apply log transformation on label 'trip_time' -> this is used to help compute RMSLE\n",
    "    model_sdf = model_sdf.withColumn(label, F.log(model_sdf[label]))\n",
    "\n",
    "    # display the features and targets for our model\n",
    "    model_sdf.select('features').head(5), model_sdf.select('trip_time').head(5)\n",
    "\n",
    "    # preprocess for predictions\n",
    "    predict_sdf = preprocess.transform(test_sdf)\n",
    "    predict_sdf = predict_sdf.withColumn(label, F.log(predict_sdf[label]))\n",
    "    predict_sdf.show(1, vertical=True)\n",
    "\n",
    "    return model_sdf, predict_sdf\n",
    "\n",
    "model_sdf, predict_sdf = preprocess_attribute(train_sdf, test_sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model and apply to the test data\n",
    "def train_predict(model, train, test):\n",
    "    model = model.fit(train)\n",
    "    predictions = model.transform(test)\n",
    "    # predictions.show(1, vertical=True)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate RMSLE and R-squared for a given model\n",
    "def evaluate(predictions):\n",
    "    # Select (prediction, true label) and compute test error\n",
    "    evaluator = RegressionEvaluator(labelCol=\"trip_time\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "    rmse = evaluator.evaluate(predictions)\n",
    "    print(\"Root Mean Squared Logarithmic Error (RMSLE) on test data = %g\" % rmse)\n",
    "\n",
    "    evaluator = RegressionEvaluator(labelCol=\"trip_time\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "    r2 = evaluator.evaluate(predictions)\n",
    "    print(\"R Squared on test data = %g\" % r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# produce residual plot for trips involving JFK airport in sampled test data (sampling data for visualisation purpose)\n",
    "def residual_plot(predictions, model_name):\n",
    "    SAMPLE_SIZE = 0.001\n",
    "    sample_prediction = predictions.select(\"trip_time\", \"prediction\", \"JFK_trip\").sample(SAMPLE_SIZE, seed=0).toPandas()\n",
    "    print(\"Number of instances in sample dataset:\", sample_prediction.shape[0])\n",
    "\n",
    "    # filter trips involving JFK airport\n",
    "    jfk_trip_prediction = sample_prediction.loc[sample_prediction[\"JFK_trip\"]==True]\n",
    "    jfk_trip_prediction[\"residuals\"] = jfk_trip_prediction[\"trip_time\"] - jfk_trip_prediction[\"prediction\"]\n",
    "    print(\"Number of instances in JFK dataset:\", jfk_trip_prediction.shape[0])\n",
    "    \n",
    "    sns.scatterplot(data=jfk_trip_prediction, x=\"prediction\", y=\"residuals\")\n",
    "    plt.title(\"Residual Plot for Sampled Trips To or From JFK Airport\", fontsize=13)\n",
    "    plt.xlabel(\"Predicted Log transformed trip time\", fontsize=12)\n",
    "    plt.ylabel(\"Residuals\", fontsize=12)\n",
    "    plt.savefig(f\"../plots/{model_name}_residual_plot.png\", bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning\n",
    "Hyperparameter tuning is done through comparing evaluation results of models with different parameter values. Due to limited computing power and large training set, we cannot perform Cross Validation on all models. \n",
    "- Gamma GLM: models trained with different values of regularisation parameter (0.01, 0.1, 0.3, 0.5, 0.7, 0.9)\n",
    "- Random Forest Regression: models trained with different number of trees (3, 5, 10, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gamma GLM \n",
    "reg_param_list = [0.01, 0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "for i in reg_param_list:\n",
    "    print(\"regParam =\", i)\n",
    "    glr = GeneralizedLinearRegression(featuresCol='features', labelCol='trip_time', \n",
    "            family=\"gamma\", link=\"log\", maxIter=10, regParam=i)\n",
    "    predictions = train_predict(glr, model_sdf, predict_sdf)\n",
    "    evaluate(predictions)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Regression\n",
    "num_tree_list = [3, 5, 10, 12]\n",
    "for i in num_tree_list:\n",
    "    print(\"numTrees =\", i)\n",
    "    rf = RandomForestRegressor(featuresCol='features', labelCol='trip_time', numTrees=i, maxDepth=5, seed=0)\n",
    "    predictions = train_predict(rf, model_sdf, predict_sdf)\n",
    "    evaluate(predictions)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Regressor\n",
    "Optimal model: numTrees=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the training data with the optimal model (5 trees) found in hyperparameter tuning\n",
    "rf = RandomForestRegressor(featuresCol='features', labelCol='trip_time', numTrees=5, maxDepth=5, seed=0)\n",
    "predictions = train_predict(rf, model_sdf, predict_sdf)\n",
    "evaluate(predictions)\n",
    "residual_plot(predictions, \"RF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalized Linear Regression\n",
    "Optimal model: regParam=0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # fit the training data with the optimal model (regParam=0.9)\n",
    "glr = GeneralizedLinearRegression(featuresCol='features', labelCol='trip_time', \n",
    "            family=\"gamma\", link=\"log\", maxIter=10, regParam=0.9)\n",
    "\n",
    "predictions = train_predict(glr, model_sdf, predict_sdf)\n",
    "evaluate(predictions)\n",
    "residual_plot(predictions, \"GLM\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "703e593df40508a60fa363339ca2bbb5bae045b0a530fb0e89bc3e7c255f1da9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
